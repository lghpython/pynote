### 关于爬虫

- requests模块:
  1. 指定url
  2. 发起请求
  3. 获取响应数据
  4. 数据解析
  5. 持久化存储
- get/post作用
  - 发起请求
  - get/post参数
    - url
    - data/params
    - headers
    - proxies 请求代理的设置
- 处理ajax动态加载的数据
  - 动态加载的数据:通过另一个请求得到的数据
  - 判断是否为动态加载的数据:基于抓包工具进行局部搜索
    - 搜索失败证明为动态加载
  - 捕获动态加载的数据:基于抓包工具进行全局搜索
    - 搜索失败是加密数据
- 抓包工具:
  - fiddler
  - 青花瓷
  - miteproxy
- 模拟登陆: 捕获登陆可见的数据
  - 处理验证码和cookie
- 打码平台使用: 对不同类型验证码进行识别
  - 超级鹰
  - 云打码
- cookie的处理:
  - 自动: session对象 requests.Session()
  - 手动: 将抓包工具中的cookie作用到headers
- 代理ip: 
  - requests : proxies参数
  - scrapy: 中间件
  - 自己构建一个免费的代理池
- 线程池: dummy.Pool.map(func,alist)
- 多任务的异步协程
  - 使用异步就都使用异步的代码
  - 特殊的函数: 使用async修饰的函数的定义
  - 协程: 特殊的函数被调用之后,返回一个写成对象函数内部的程序语句不会被立即执行
  - 任务对象:
    - 一个高级的协程对象
    -  绑定回调函数
  - 事件循环: 任务对象需要注册到时间循环对象中,时间循环开启后其内部注册的任务对象会被逐一的异步执行
  - await: 挂起
  - async: 特殊函数
  - aiohttp: 支持异步网络请求的模块
    - 基本的架构
    - 补充细节
- 图片懒加载: 伪属性

### 数据解析: 实现聚焦爬虫

- 正则表达式
- bs4
- xpath
- pyquery

### selenium : 基于浏览器自动化的模块

- 关联: 
  - 获取动态加载的数据
  - 实现模拟登陆
- 使用流程
  - 浏览器的驱动程序
- find系列函数
  - 标签定位: 
- send_keys:
- click
- page_source
- switch_to.frame函数
  - 使用find系列的函数进行标签定位的时候如果剖出NoSuchElementException. 定位的标签存在与一个iframe(嵌套的子页面)标签
- js注入:
  - execute_script(jsCode)
- 动作链:
- phantomJs:
- 谷歌无头浏览器
- Pyppteer: 一个异步的selenium

### appnium: 基于手机自动化的模块

### scrapy异步框架(Twisted) 

- 项目的创建流程:
  - 创建工程: scrapy startproject xxxPro
  - cd xxxpro
  - 创建爬虫文件:scrapy genspider spoiderName www.xxx.com
    - 父类:Spider
    - name
    - start_url
    - parse(response)
  - 持久化存储:
    - 基于终端指令
    - 基于管道
      - 解析数据
      - 将解析的数据密封到item对象中
      - 将item提交给管道
      - 在管道中进行任意形式的持久化存储
      - 在配置文件中开启管道
  - 处理分页数据爬取(全站)
    - 手动请求的发送 : yield scrapy.Request(url,callback,meta)
  - cookie处理:
    - 自动处理:settings配置文件中Cookie_Enable = True
  - 日志登记:
    -  LOG_LEVEL = 'ERROR'
  - 请求串餐
    - 作用:实现深度爬取
    - 使用场景: 爬取数据没有在同一张页面
    - 串餐:在请求时传递item对象
    - yield  scrapy.Request(url,callback,meta={}):将meta字典传递给callback
    - 在callback中接收meta字典: response.meta
  - 五大核心组件原理
    - Spider: 生成url,数据解析
    - 引擎: 处理数据流的事务的触发
    - 调度器:取出重复的请求对象,跳读对应的请求对象进行数据下载
    - 下载器: 接收请求对象,进行数据的异步下载
    - 管道: 持久化存储
  - 下载中间件: process_request,process_response,process_exception(return request)
    - 作用: 拦截请求和响应
    - 拦截请求:
      - 篡改请求头信息
      - 代理
  - UA池和代理池:
  - selenium在scrapy中的应用
  - cravlSpider
    - 高效的实现全栈数据的爬取
    - LinkExtractor:根据指定的规则(正则)对指定的连接进行提取
    - Rule:接收LinkExtractor提取到的连接,对其进行请求发送,然后根据指定规则进行数据解析
    - 注意: cravlSpider中进行深度爬取时不可以使用请求传参
- 分布式
  - scrapy不能实现分布式: 调度器和管道不可以被共享
  - scrapy-redis的作用:给scrap 与提供可以被共享的调度器和管道
  - 实现分布式的方式
  - 流程
- 增量式爬虫: 监测网站内容的更新情况
  - 去重,redis的set集合 

